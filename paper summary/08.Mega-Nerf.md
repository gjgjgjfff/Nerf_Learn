> [参考](https://drive.google.com/drive/folders/1jIr8dkvefrQmv737fFm2isiT6tqpbTbv)这里

# 要解决的问题
* 大型场景Nerf往往面临着不同的光照条件
* 训练速度慢、不能实时渲染

# 贡献
* 把训练集划分成了更小的子集
* 把场景划分为很多区域，每个区域训练一个Nerf，每个Nerf的训练只需要上面所说的划分过的数据集，且每个Nerf的训练是可以并行的(即一张GPU训练一个)
* 提出一种新的渲染方式，比传统的更快
* 提出了一个新的大场景数据集，包含从一个工业园区附近100,000 ${m^2}$ 无人机拍摄的数千张高清图像

# 具体实现
## 一、训练
### 1.场景划分
![Spatial-partitioning](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Mega-Nerf/Spatial-partitioning.png)
其中 ${f^n}$ 就是一个Nerf，对于每个点具体是选哪个Nerf？是取距离该点最近的Nerf。  
其中，中心点的选择方式，论文中说按照重要性什么的，不过代码里是直接按照空间进行了一个简单的划分。
### 2.训练集划分
场景划分完成之后，需要对训练集按照我们切分的地图也进行一个划分，对于每张图片，当穿过这张图片某个像素的光线和某个cell相交时，就把该图片加入对应的cell的训练集中。例如，对于从无人机2处观察这张图片，就把它加到cell A、B、F的训练集中。  
![drone2](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Mega-Nerf/drone2.png)  
还有个剪枝操作，当训练一段时间(epoch=100,000)后，我们可以根据density信息裁剪掉对这个子模块没有贡献的训练集，例如，对于从无人机2处观察这张图片，但在F处有一堵墙，那么就可以将该图片从cell A、B的训练集中删除。    

另一个角度解释为什么要进行训练集的划分：大规模场景中单张图片所捕获的场景百分比明显低于传统Nerf中单张图片所捕获的场景百分比，所以要对训练集进行一个划分。
![Scene-Captured](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Mega-Nerf/Scene-Captured.png)
### 3.前景和后景的分别处理
![Foreground-and-background-decomposition](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Mega-Nerf/Foreground-and-background-decomposition.jpg)
用椭球代替Nerf++的球体实际上对结果的提升并不大，而且在某些场景(例如相机的运动轨迹没有那么复杂的情况)下还可能出错(即把大部分的场景都放到了后景渲染，而后景的渲染能力较弱)
## 二、渲染


# 局限性
* 没有全局优化，所以必须有精确的相机位姿，否则各个cell在边界上会有不对应的情况（是通过一个sfm预处理过的的相机位姿输入到网络的，没有预处理的相机位姿直接输入网络效果会很差）
* 没有考虑场景的动态物体（在训练之前预处理了整个数据集，让动态物体没有出现在训练中）
* 无法实时渲染
* 没有突破Nerf的训练速度瓶颈



