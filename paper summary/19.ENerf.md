# 要解决的问题
输入是多个相机在固定机位拍摄的某个动态场景的多目视频，论文希望能生成该动态场景的自由视点视频(即其他视角的视频)
# 当前方法在这个问题上的局限性
为了支持自由视点视频的应用，自由视点视频的渲染效果需要足够逼真，生成制作需要足够快，生成后在用户端的渲染也需要足够快。最近一些方法基于隐式神经表示，利用体渲染技术优化场景表示，从而制作自由视点视频。如D-NeRF利用隐式神经表示恢复了动态场景的动作，实现了照片级别的真实渲染，但是，这一类方法很难恢复复杂场景的动作，且他们训练一个模型需要从几小时到几天不等的时间，此外，渲染一张图片通常需要分钟级的时间。
# 本文的观察和对问题的解决
我们观察到通过MVS方法预测显式表示，例如深度图，通常是很快的。利用此显式表示去引导隐式表示的体渲染过程中的采样，能够大幅降低此前基于Nerf的方法在空间内密集采样点（包括空气地方的点和被遮挡的点）造成的计算开销，从而实现加速。
# 具体实现（粗略看了下，具体细节还要看代码）
本文引入深度引导采样策略，大大加快基于Nerf的方法的渲染速度，我们首先从输入图像中提取多尺度特征图，用于构建cascade cost volume，之后根据cascade cost volume得到3D feature volume和coarse scene geometry（用深度图和置信度图来表示），这些深度图可以指导我们尽量在物体表面周围采样点，显著加速渲染过程，3D feature volume为Nerf的构建提供了丰富的几何感知信息。仅使用RGB图像对所有网络组件进行端到端训练。  
> 提问：什么是cascade cost volume？  
> 提问：什么是feature volume？  

![pipline](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/ENerf/pipline.png)  
