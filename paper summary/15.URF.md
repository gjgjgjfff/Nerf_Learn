# 要解决的问题
利用RGB图像和激光雷达扫描数据（由摄像机和扫描仪在室外场景中移动获得），进行三维重建和新颖的视图合成，适用于城市户外环境中的世界地图绘制(如街景)。

# 创新点，主要是后两个创新点对新视图合成有帮助，第一个创新点是对三维重建有帮助
* 除了RGB图像外，纳入了激光雷达信息。通过融合这两种模式，可以弥补在大规模和复杂的场景中视点的稀疏性。同时引入了一系列基于激光雷达的损失，允许对固体结构(如建筑物)和体积结构(如树木/植被)进行精确的表面估计
* 自动分割天空像素，并定义一个单独的圆顶状结构，为指向天空的相机射线提供一个明确定义的监督信号
* 通过估计每个相机的仿射颜色变换自动补偿不同的曝光

# 局限性
每一篇针对大规模场景的NeRF都对每幅图像运行预先训练的语义分割模型(一般是Mask_RCNN)，然后屏蔽人的像素(因为人是最突出的移动类别)，因此都不能处理动态场景。

# 具体实现，[参考这里](https://zhuanlan.zhihu.com/p/606394614)
总体优化目标：  
![all_loss](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/URF/all_loss.png)  
网络pipline  
![network-pipline](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/URF/network-pipline.png)  
## 一、Photometric-based Losses
这个Loss和NeRF本身的Loss差不多，但是模型的C中加入了exposure parameters 。同时体渲染的公式也被改了一下，加入了天空和曝光度颜色校正。  
![Photometric-based-Losses](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/URF/Photometric-based-Losses.png)  
### 1.Exposure compensation(曝光补偿)
之前的方法总是尝试学习每一张照片单独的曝光能力（Appearance Embedding），这就被过参数化了。本篇文章的解决方法是利用一个网络生成一个3*3的矩阵，用来对图片的颜色做仿射变换  
> 思考：其实曝光或者光照本质上就是对颜色的一种仿射变换，因此让那个模型预测一个仿射变换矩阵即可解决曝光问题？
### 2.Sky modeling(天空建模)
天空场景由于没有和任何不透明的表面有交点，因此NeRF很难得到有监督的信号，所以单独建模天空，对于天空，只输入方向信息d(不需要位置坐标x)，输出颜色c？  
![sky-model](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/URF/sky-model.png)  
在解决这个问题时首先也跑一个预训练的语义分割模型，让语义分割模型判断哪个pixel是天空( ${S_i}(r) = 1$ 表示光线r穿过图像i中的天空像素)，同时加入一个Loss，鼓励模型对天空背景的体密度预测为0。  
![sky-loss](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/URF/sky-loss.png)  
> 思考：是不是可以理解为，当 ${S_i}(r) = 1$ 时，要想让式(11)最小，就相当于让 $w(t)$ 趋向于0，之后式(8)就只剩下 ${c_{sky}}(d)$ 了？
## 二、Lidar loss
