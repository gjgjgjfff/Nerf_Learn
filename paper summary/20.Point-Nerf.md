# 要解决的问题
NeRF系列针对每个场景从头开始重建太过耗费时间，并且还对很多大片的空旷区域进行了不必要的采样。作者观察到基于学习的multi-view stereo（MVS）可以快速地重建一些场景，所以用基于MVS的方法得到神经点云来指导NeRF，大大加快重建速度。

# 具体实现
Point-NeRF结合了NeRF、3D点云和MVS中通过卷积神经网络快速重建场景的思路，先通过MVSNet得到初始神经点云场景，然后利用神经3D点云和图像特征来构建基于点的辐射场，最后利用Point-Net类似的多个MLP网络结构回归得到体密度和r，大大加快重建速度，此外还提出优化基于点的辐射场的方法点剪枝和点生长。(感觉是参考Nerf论文的结构，先提出场，然后给出优化方法)  
Point-Nerf可以分为两个部分，生成初始的基于点的辐射场(Generating initial point-based radiance fields)和优化基于点的辐射场(Optimizing point-based radiance fields)  
![pipline](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Point-Nerf/pipline.png)  
## 一、生成初始的基于点的辐射场(Generating initial point-based radiance fields)
### 1.点位置和置信度
对于给定的不同视角的图片，首先用MVSnet生成的深度图构建出点云 ${p_i}$ ，然后在前面构成的深度概率体上三维线性地采样来获得每个点代表场景表面的置信度 ${\gamma _i}$ ，可以公式化为：  
![Point-location-and-confidence](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Point-Nerf/Point-location-and-confidence.png)  
${I_q}$ 和 ${\Phi _q}$ 

分别代表视角 $q$  的输入图像和相机参数， ${I_{q1}}$ 和 ${\Phi _{q1}}$ ...代表MVS重建中使用的附加相邻视图，在大多数情况下，使用两个附加视图。
### 2.点特征
我们使用2DCNN ${G_f}$ 从每个图像 ${I_q}$ 中提取图像特征图，这些特征图与来自 ${G_{p,\gamma }}$ 的深度预测对齐，并用于直接预测每点特征 ${f_i}$，可以公式化为：  
![Point-feature](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Point-Nerf/Point-feature.png)  
论文中用具有三个下采样层的VGG网络构成 ${G_f}$ ，将不同分辨率的中间特征组合为 ${f_i}$ 。  
通过以上两个步骤就构建了基于点的辐射场：  
![Point-based-radiance-field](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Point-Nerf/Point-based-radiance-field.jpg)  
### 3.新试图合成
Nerf是从辐射场中，利用MLP网络结构回归得到体密度和r，Point-Nerf是从基于点的辐射场，利用Point-Net类似的多个MLP网络结构回归得到体密度和r。  
我们的基于点的辐射场可以抽象为神经模块，给定任何3D位置x，该神经模块从其相邻的K个神经点回归任何采样点位置x处的体密度σ和rgb。  
总的来说，文章采用了类似于PointNet的，拥有多个子MLP(即下图的F、T、R)的神经网络来做回归，首先对每个神经点进行神经处理，然后聚合多点信息以获得最终的估计得到体密度和r。可以公式化为：  
![point-nerf](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Point-Nerf/point-nerf.png)  
具体网络结构如图所示：  
![point-net](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Point-Nerf/point-net.png)  
## 二、优化基于点的辐射场(Optimizing point-based radiance fields)
对于输入的现成点云，初始点云，特别是来自外部重建方法(如图metshape或COLMAP)的点云，通常会包含孔洞和异常值，从而降低渲染质量。针对这一问题，在逐场景优化过程中，我们发现直接优化现有点的位置会导致训练不稳定，无法填补大的空洞，因此，我们采用了新颖的点剪枝和生长技术，逐渐提高了几何建模和渲染质量。
### 1.点剪枝
${\gamma _i}$ 来描述一个神经点是否靠近场景表面，本文利用这些置信度来删除异常点，注意，点置信度与体密度回归中每点的贡献直接相关(Eqn.7)，因此，低信度反映了一个点的局部区域体积密度低，表明该点是空的。我们每10K次迭代就删除 ${\gamma _i}$ < 0.1的点，这种剪枝操作可以去除异常点。本文还设计了一个置信度的loss，迫使置信度趋向于0或者1：  
![Point-pruning](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Point-Nerf/Point-pruning.png)  
### 2.点生长
我们还提出了一种新的技术来增长新的点，以覆盖原始点云中丢失的场景几何，与直接利用来自现有点的置信度信息的点剪枝不同，生长点需要在没有点存在的空区域中恢复点信息。具体来说，我们沿着找到每条光线采样的点中选择具有最高不透明度的点(即体密度最大的点) ${x_{jg}}$ ，如果该点的不透明度大于阈值且其到最近的神经点的距离也大于阈值，则表明该点位于场景表面附近，但远离其他神经点，则在 ${x_{jg}}$ 位置生长一个神经点，不透明度公式就是Nerf体渲染中的：  
![Point-growing-opacity](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Point-Nerf/Point-growing-opacity.png)  
通过重复这种grow策略，我们的辐射场可以扩展到覆盖初始点云中缺失的区域。下图表明即使在只有1000个初始点的极端情况下，该grow技术也能够逐步生成新的点并合理地覆盖物体表面：  
![Point-growing](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Point-Nerf/Point-growing.png)  
