# 要解决的问题
Nerf应用在动态场景中，一个容易想到的解决方案是直接以6D的信息作为Nerf的输入(包括位置坐标(x,y,z)、方向向量($\theta$ $\phi$)、时间(t)，当然，都是要经过PE的，下面的也都是)，得到颜色 $c$ 和体密度 $\sigma$ ，但这样做没有很好的利用时间冗余，效果并不好。
> "时间冗余"（Temporal Redundancy）是指信息或数据在时间上的重复出现或重复传输，这种重复性通常是出于数据冗余性的目的，以提高数据的可靠性、容错性或性能。时间冗余的应用有助于降低数据丢失、传输错误和数据损坏的风险，然而，它也可能会增加存储和传输的开销。  

# 具体实现
整个网络分为两部分，一个规范网络(Canonical Network，其实就是t=0时刻场景的Nerf)和一个变形网络(Deformation Network)，变形网络预测将 时间t的场景中某点x 转换为 规范图像(t=0时刻)相应点 所需的偏移量 $\Delta x$ ，然后规范网络预测点 $x + \Delta x$ 的颜色和体密度。网络的pipline为：  
![pipline](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/D-Nerf/pipline.png)  
可以公式化为：  
![mapping](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/D-Nerf/mapping.png)  
## 一、模型
### 1.规范网络(Canonical Network)
其实就是一个t=0时刻的场景的Nerf，可以公式化为：  
![Canonical-Network](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/D-Nerf/Canonical-Network.png)  
其中，$x$ 是给定t时刻的3D点坐标，$\Delta x$ 为经过变形网络得到的变换， $d$ 为变换后的规范场景下的点 $x + \Delta x$ 所对应的视角。
### 2.变形网络(Deformation Network)
变形网络预测时间t时刻的场景与规范场景(t = 0)之间的变换，即将 时间t的场景中某点x 转换为 规范图像(t=0时刻)相应点 所需的偏移量 $\Delta x$ ，可以公式化为：  
![Deformation-Network](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/D-Nerf/Deformation-Network.png)  
## 二、体渲染
和原始Nerf的一样，只是多了变形网络的步骤，可以公式化为：  
![Volume-Rending](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/D-Nerf/Volume-Rending.png)  
当然，真实情况不可能每条射线的每个点都采样，所以一定是离散化的：  
![Volume-Rending-discrete](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/D-Nerf/Volume-Rending-discrete.png)  