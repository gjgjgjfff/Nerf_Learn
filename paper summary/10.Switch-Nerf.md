# 要解决的问题
### 现有方法的问题
设计应对大规模场景的Nerf的一种策略是：将场景分解，由不同的子网络分别处理一个场景中的不同部分。  
而之前采用类似想法的工作(Block-Nerf、Mega-Nerf)都是手工的分解场景，Mega-Nerf用3D采样距离将场景分解并训练不同的Nerf，Block-Nerf用街道块将场景分解并训练不同的Nerf，虽然在大规模场景上取得了令人满意的结果，但他们手工制作的场景分解方法仍然会导致几个问题：  
1.大型场景本质上是复杂和不规则的，以手工制作的方式为不同场景设计一个通用的场景分解规则极具挑战性，这相应地带来了现实世界中不同场景的适应问题；  
2.手工制作的规则需要目标场景的丰富先验知识，如场景的结构，以像Mega-NeRF那样部署分区质心，像Block-NeRF那样部署场景图像的物理分布，这些先验在实际应用中可能不可用；  
3.手工制作的分解是不可学习的，这阻碍了网络以端到端的方式联合优化场景分解和辐射场。分解、合成和NeRF优化之间的差距可能导致次优结果。  
### 本文提出的解决方法
设计一个门控网络动态的将3D点分配到不同的Nerf子网络中，且该门控网络可以与不同的Nerf子网络一起被优化(即场景分解和Nerf子网络一样，是可学习的)

# 相关工作
### 1.Mixture of Experts（MoE）
Mixture of Experts（MoE）是一种集成学习方法，用于将多个专家（Expert）模型组合成一个更强大的模型，它被广泛应用于解决复杂的预测和分类问题。MoE设计了一个基础的Top-k门控网络将样本分配给k个专家（即激活k个部分的参数，Top-1就是只激活一个部分的参数），并提出一个辅助函数来平衡不同专家的训练。整个Mixture of Experts模型可以描述为以下几个组成部分：  
* 专家模型（Expert Models）：每个专家模型是一个独立的机器学习模型，可能是决策树、神经网络、线性模型等。每个专家模型都在数据的特定子空间中表现较好。
* 门控网络（Gate Network）：门控网络是一个用于决定选择哪个专家模型的模型。它的输入通常是原始输入数据，并根据数据的不同特征来决定激活哪个专家模型。可以使用神经网络来实现门控网络。
* 混合权重（Mixture Weights）：混合权重是由门控网络计算得出的权重，用于加权各个专家模型的输出。权重决定了每个专家模型对最终输出的贡献程度。  
### 2.他为什么会想到用MoE呢？
因为将场景中的3D点分配给不同的Nerf的过程(即门控网络) 和 Nerf本身的优化是一对离散的操作，所以如何将梯度反向传播到门控网络是一个问题，所以想到用MoE。

# 具体实现
## 一、pipline
3D点x将首先通过门控网络，然后根据门控网络输出仅被分派给一个专家。专家输出与相应的门值相乘，并被发送到head，用于密度 $\sigma $ 和颜色c预测。渲染损失用于监督。  
![pipline](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Switch-Nerf/pipline.png)  
## 二、门控网络
$G(x)$ 表示输入x经过一堆线性层再通过softmax的n维向量， 则 $G{(x)_i}$ 表示选择第i个Nerf Expert的概率，用 ${E_s}$ 表示选到Nerf Expert，则输入到head的是 $\widetilde {E(x)} = G{(x)_s}{E_s}(x)$ 。当预测的门值被乘以NeRF Expert的相应输出时，门控网络可以在backward中与NeRF Expert一起被优化。  
![Gate-network](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Switch-Nerf/Gate-network.jpg)  
## 三、NeRF Expert和head
见上图即可
## 四、容量系数的分配方法和全分配
### 1.训练的时候，用容量系数的分配方法将点分配
通过容量系数，限制分配给每个Nerf Expert的采样点数，如果有多余的点，将会被丢弃，如果某个Expert没有被装满，则用0填充，本文将容量因子设置为1.0。  
![capacity-factor](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Switch-Nerf/capacity-factor.png)  
这样做保证了每个Nerf Expert有相同维度的Tensor。
### 2.测试的时候，用全分配
以前的MoE方法通常对训练和测试都使用容量系数的调度方法，这不可避免地会减少样本点，从而导致结果精度下降。在本文中，将所有的点分配给相应的Nerf Expert。
## 五、体渲染和损失
### 1.体渲染
和最原始的Nerf体渲染一样
### 2.损失
损失包括两部分，一部分是体渲染的损失，这部分和最原始的Nerf一样。  

另一部分是辅助损失，用于平衡优化，训练基于MoE的网络的一个问题是，门控网络只偏向于少数NeRF Expert，因此，NeRF Expert的优化和利用将是不平衡的，甚至可能导致几个NeRF Expert没有被训练，然后整个网络收敛到次优解决方案，为此，参考Shazeer等人(2017)；Lepikhin等人(2021)的工作，我们使用一个辅助损失 ${L_a}$ 来正则化门控网络并平衡NeRF Expert的利用率。具体来说，根据GShard (Lepikhin等人，2021年)提出的可区分负载平衡损失，我们将辅助损失定义如下。  
![loss](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/Switch-Nerf/loss.jpg)  
