> 先了解一些

> 频率/光谱误差(frequency bias)：  
>一方面在样本密度（相对于频率）分布均匀，网络会更快地学习到低频特征，而学习到高频特征则需要更多的训练时长；另一方面当样本集中分布在高频特征区域时，网络极有可能优先学习出高频特征（即优先学习得到样本密度高的区域的特征）。这里要注意的是低频信号的泛化性要更强。

>NeRF学习过程中高低频信号分别是什么？  
>从 geometry 的角度出发，低频信号是目标场景的轮廓（比如球形之于苹果），高频信号则是轮廓上细微的形状（比如苹果的柄和两端的凹陷）；  
>从 appearance 的角度出发，低频信号是 diffuse，高频信号就是 specular。

# 要解决的问题
用少量视角的图片进行神经渲染，从频率的角度解决少量视图进行神经渲染的问题，具体来说，提出两个正则化方法，分别是频率正则化和遮挡正则化。

# 具体实现
### 一、Frequency Regularization(频率正则化)
这里仅针对 geometry 部分，因为 NeRF degeneration 主要表现是 floater（类似云雾的效果），这显然是因为 geometry 没优化好导致的。而正如上面提到的 geometry 的低频信号对应的是目标场景的轮廓，而轮廓信息恰恰需要多视角提供；而高频信息往往只出现在少数几个视角里（因为越细致的形状越难反映在多视角中）。因此给定少视角意味着低频信号的样本密度低，同时由于NeRF采用了 Positional Encoding （包含了高频信号的基函数），少视角中包含的高频信号（细致的形状）反而会更容易优化出来，因为较高频率的映射能够加快高频分量的收敛速度，然而，高频上的过快收敛阻碍了NeRF探索低频信息，并使NeRF明显偏向不期望的高频伪像，因此，我们假设高频成分是在少数镜头神经渲染中观察到模糊图像的主要原因。  
而作者为了解决这一问题，作者提出了一个 Positional Encoding 的退火策略：  
![Frequency-Regularization](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/FreeNerf/Frequency-Regularization.png)  
上面这个公式比较抽象，可以看下面这个代码，简单来说就是从没有位置编码的原始输入开始，随着训练的进行（训练步长t的增加），逐渐开放 Positional Encoding 中高频信号的参与。这样一来就强迫 NeRF 在训练的早期优化低频部分，从而实现更好的泛化性能。  
![Frequency-Regularization-code](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/FreeNerf/Frequency-Regularization-code.png)  
### 二、Occlusion Regularization(遮挡正则化)
频率正则化不能解决少镜头神经渲染中的所有问题。由于有限数量的训练视图和问题的不适定性，某些特征伪像可能仍然存在于新视图中,这些故障模式通常表现为距离摄像机非常近的“墙”或“漂浮物”,这种现象大多来自训练视图中重叠最少的区域。简单来说就是少视角情况下会出现，密度 $\sigma $ 集中分布在少视角的相机近平面的情况。 这个现象其实很好解释。根据 ${\rm{x}} = o + dt$ ，每个视角都对应着一个视锥（视锥是随着 $t$ 外扩的），而当我们对视角的光线进行样本点采样时，一样的 $t$ 意味着这些样本点落在这个视锥的 $t$ 对应的截断平面上。而当 $t$ 越小，这一截断平面越小，这也意味着落在这一截断平面的样本点密度越高。所以对单个视角而言，落在近平面的样本点密度是最大的，那么网络就会认为近相机平面的密度很大，即网络是很“懒”的，当我们不添加正则来“鞭策”它的话，它就会倾向于用最小成本的方式学习到“凑合”的解（“凑合”意味着尽可能满足训练样本，陷入局部最优解）。 所以在少视角的情况下，NeRF 就会倾向于在近平面产生一个凑合的效果。而在少视角下产生的平面，在新视角中就会以“墙”的形式出现。 而 FreeNeRF 的处理方式也非常的简单，就是惩罚近平面附近样本点。  
![Occlusion-Regularization](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/FreeNerf/Occlusion-Regularization.png)  
${m_k}$ 是一个mask，用来判断当前的点是否要被惩罚， ${\sigma _k}$ 表示沿射线采样的K个点的密度值，按照接近原点的顺序(从近到远)，将 ${m_k}$ 的值设置到索引M(称为正则化范围)之前为1，其余为0。
