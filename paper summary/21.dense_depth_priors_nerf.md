# 要解决的问题
利用深度信息来优化Nerf，从而用少量的输入视图就能合成高质量的indoor级别的新视图。

# 具体实现
> 本文采用深度补全+Nerf  

首先通过SFM/colmap获得稀疏点云，把稀疏点云投影到图上得到稀疏深度图，再通过一个深度补全的网络得到稠密的深度图和不确定性图(不确定性比较大的地方集中在物体边缘地区/近地面地区)，这两种图指导Nerf，使得Nerf收敛的更快。
![pipline](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/dense_depth_priors_nerf/pipline.png)  

> 为什么要得到稠密的深度图？
> 因为稀疏深度图中只有很少的像素包含有效的深度信息，例如用18~20张ScanNet图片，用SFM重建的稀疏深度图中，只有0.04%的像素包含有效深度信息，而稠密深度图是指每个像素都包含有效深度信息。
> ![depth](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/dense_depth_priors_nerf/depth.jpg)  
## 一、深度补全网络--这部分没细看，主要看第二部分，假设我们已经得到了稠密的深度图和不确定性图
## 二、具有密集深度先验的辐射场
因为视角很少，所以省略了视角方向的位置编码，目的是减少rgb和体密度对输入视角回归的依赖，同时，输入Nerf的加上每幅图像的embedding vector，允许网络补偿特定视图的现象，如不一致的照明或者镜头阴影。  
![nerf](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/dense_depth_priors_nerf/nerf.png)  
### 用体渲染权重计算深度和不确定性--我也不知道为什么能这么算
![depth-calculate](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/dense_depth_priors_nerf/depth-calculate.png)  
式中 ${w_k}$ 指渲染的权重， ${t_k}$ 指射线上采样点的深度，之后用color和depth两个loss联合监督Nerf：  
![color-loss](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/dense_depth_priors_nerf/color-loss.png)  
![depth-loss](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/dense_depth_priors_nerf/depth-loss.png)  
### 深度指导的采样
和Nerf一样是分层采样，训练时，每个像素点的深度值 $z(r)$ 和 不确定性 $s{(r)^2}$ 是已知的，则前一半的采样点和Nerf一样从near平面到far平面等间隔采样，后一半采样点是在以 $z(r)$ 为均值， $s{(r)^2}$ 为方差的高斯分布 $N(z(r),s{(r)^2})$ 进行采样。测试时，深度值和不确定性是未知的，则前一半的采样点和Nerf一样从near平面到far平面等间隔采样，同时得到预测的深度值 $\hat z(r)$ 和 不确定性 $\hat s{(r)^2}$ ，后一半采样点是在以 $\hat z(r)$ 为均值， $\hat s{(r)^2}$ 为方差的高斯分布 $N(\hat z(r),\hat s{(r)^2})$ 进行采样。
![sampling](https://github.com/gjgjgjfff/Nerf_Learn/blob/main/img/dense_depth_priors_nerf/sampling.jpg)  
